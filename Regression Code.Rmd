---
title: "R Notebook"
output: html_notebook
---

Grade point average. The director of admissions of a small college selected 120 
students at random from the new freshman class in a study to determine whether 
a studentâ€™s grade point average (GPA) at the end of freshman year (Y) can be 
predicted from the ACT test score (X). The results of the study follow. Assume 
that first-order regression model (1.1) is appropriate.

```{r Simple Linear Regression}

# read data
gpa <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 20/6450/Assignment_1/gpa.txt", header = TRUE)

# fit linear model + print summary
gfit <- lm(GPA ~ ACT, data=gpa)
summary(gfit)

# plot data with fitted regression line
plot(gpa$ACT, gpa$GPA, cex=.5, main="Data and Fitted Regression Model", ylab="Freshman GPA", xlab="ACT Score")
abline(gfit,lwd = 3)

# produce confidence interval for B_1
confint(gfit, 'ACT', level = 0.99)

# t-test for relationship
beta0 <- coef(gfit)[1] #coefficient b_0
beta1 <- coef(gfit)[2] #coefficient b_1
fitted_values <- beta0 + beta1 * gpa$ACT #y_hat values

sse <- sum((gpa$GPA - fitted_values) ^2) #sse
mse <- sse / (nrow(gpa)-2) #mse
sigma.hat<-sqrt(mse) #estimated standard dev
sxx <- sum((gpa$ACT - mean(gpa$ACT))^2) #msr
se.beta.hat.1 <- sqrt(mse / sxx) #standard error
T.statistic <- beta1 / se.beta.hat.1 #t statistic
p_value <- 2*pt(T.statistic, df=nrow(gpa)-2, lower.tail=FALSE)
t.star <- qt(.995, nrow(gpa)-2) #t*

# estimate mean y value
newdata <- data.frame(ACT = c(28))
ci <- predict(gfit, newdata, interval = "confidence", level = .95)

# predict y for known x value
pi <- predict(gfit,newdata,interval = "predict", level = .95)


# ////////////////////////////////// Answers //////////////////////////////////
# Least Squares Estimates
# $\hat{\beta}_{0} = 2.11405$
# $\hat{\beta}_{1}=0.03883$
# 
# Equation of a Line
# $\hat{Y} = 2.11405 + 0.03883X$
# 
# Describe Plot
# The estimated regression function appears to fit the data poorly. The 
# majority of the data appears to show a stronger relationship between ACT 
# score and freshman GPA; however, points (29, 0.5) and (31, 1.486) among a 
# handful of others prevent the regression function from having a steeper slope.
# 
# Interpret Confidence Interval
# If the observational study were repeated again and again, the coefficient for 
# B_1 is expected to be in theconfidence interval, calculated with this method, 
# 99% of the time.
# If the CI included 0, the director of admissions could ignore ACT score as a 
# predictor of Freshman GPA since the results would suggest the observed 
# relationship may be due to error.
#
# Hypothesis Test for Relationship
# H_0: B_1=0 
# H_a:B_1 \neq 0
#
# I will be able to reject the null hypothesis if T < -2.618137 or T > 2.618137
# The t-statistic is 3.04; therefore, I can reject the null hypothesis.
# 
# The P-value is 0.002. The probability of getting a T-statistic of 3.04 would 
# be less than .01 if the null hypothesis were true, satisfying the 99% 
# confidence interval. I can conclude there is a linear association between ACT 
# score and freshman GPA.
#
# Estimate Mean Y
# If the observational study were conducted again and again, the mean freshman 
# GPA for a student scoring 28 on the ACT is expected to be in the created 
# confidence interval 95% of the time.
#
# Predict Y
# The prediction interval when calculated with this method will contain the 
# student's actual Freshman GPA 95% of the time.
# 
# Note on Difference
# The prediction interval includes the prediction of error and will always be 
# wider than the confidence interval, which predicts the mean exclusively.
# 
# Additional - Relationship test
# If there were no linear association between Y and X, 0 would be included in 
# the confidence interval for slope. However, 0 is not included in the 
# confidence interval, so this conclusion is valid with a significance of 0.05.
# 
# ////////////////////////////////////////////////////////////////////////////
```


```{r Check Assumptions}

# /////////////////////////// Assumptions /////////////////////////////////////
# 1. Normality
# 2. Constant Variance
# 3. Independence
# 4. Outliers
# /////////////////////////////////////////////////////////////////////////////

# read data
gpa <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 20/6450/Assignment_2/data/CH03PR03.txt", header=TRUE)

# plot data
plot(gpa$X,gpa$Y, ylab="Freshman GPA", xlab="ACT Score", main="Scatter Plot", pch=18, col="orange")

# plot normality
hist(gpa$X, xlab="ACT Scores", ylab="Frequency of Score", main="Histogram", col="orange")

# plot residuals
gfit <- lm(Y ~ X, data=gpa) #fit linear model
gpa.e <- resid(gfit) #residuals
plot(gpa$X, gpa.e, xlab="ACT Score", ylab="Residual", main="Residual Plot", col="orange", pch=18)
abline(h=0) #horizontal line
abline(h=1, lty=c(2)) #dotted line
abline(h=-1, lty=c(2)) #dotted line

# plot QQ plot
library(car)
qqPlot(rstandard(gfit), col="orange", pch=18)

# test normality (reject if < 0.05)
shapiro.test(gpa.e)

# ////////////////////////////////// Extra ////////////////////////////////////
# View residuals against other variables to see if they should be included in
# linear model.
# ////////////////////////////////////////////////////////////////////////////

# plot residuals against unused variables
plot(gpa$V2, gpa.e, col="orange", pch=18, xlab="V2", ylab="Residuals", main="V2 Residual Plot")
abline(h=0)
plot(gpa$V3, gpa.e, col="orange", pch=18, xlab="V3", ylab="Residuals", main="V3 Residual Plot")
abline(h=0)

```

```{r ANOVA}

# read data
gpa <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 20/6450/Assignment_2/data/CH03PR03.txt", header=TRUE)

# anova table
anova(gfit)

# f-test for value of variables in model (H_0: intercept-only model)
f <- qf(.95, 1, 118) #f*
MSR <- 3.588
MSE <- 0.388
Fstar <- MSR/MSE #f statistic

# calculate r-squared
SSR <- 3.588
SSE <- 45.818
SSTO <- SSR+SSE
Rsquared <- SSR/SSTO

# not sure????
conc <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 20/6450/Assignment_2/data/CH03PR15.txt", header=TRUE)
gfit_conc <- lm(conc$Concentration ~ conc$Hours)
gfit_conc_full <- lm(conc$Concentration ~ 0 + factor(conc$Hours))
anova(gfit_conc, gfit_conc_full)
compare <- qf(.975, 3, 10)
f_star <- (2.7673/3)/(.1574/10)

# ////////////////////////////////// Answers //////////////////////////////////
# F Test
# Null Hypothesis: H_0: B_1=0
# Alternate Hypothesis: H_a: B_1 \neq 0
# Because the test statistic is larger than F*, I can reject the null hypothesis 
# with 95% confidence. I conclude that the full regression model is better than 
# an intercept-only model.
# 
# Unknown Test????
# H_0: \hat{Y} = -0.3240X + 2.5753$; H_a: \hat{Y} \neq -0.3240X + 2.5753
#
# This information does not indicate what regression function is appropriate, 
# since all other regression functions are included in the alternate hypothesis. 
# If we want to explore alternate regression functions, we can look to other 
# diagnostics and examine the residuals.
# 
# /////////////////////////////////////////////////////////////////////////////
```

```{r Transformations}

# read data
sales <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 20/6450/Assignment_2/data/CH03PR17.txt", header=TRUE)

# view boxcox plot
gfit <- lm(Y ~ X, data=sales)
library(MASS)
boxcox(gfit)

# transform data
gfit_trfm <- lm(sqrt(Y) ~ X, data=sales)
summary(gfit_trfm)

# ////////////////////////////////// Answers //////////////////////////////////
# Box-Cox Plot
# The Box-Cox procedure suggests a lambda = .5 transformation would be 
# appropriate.
# 
# Interpret Transformed Data
# For each one-year increase, the expected change in square root of sales in 
# thousands is 1.07629.
#
# /////////////////////////////////////////////////////////////////////////////
```

```{r Multiple Linear Regression}

# read data
patient <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 20/6450/Assignment_3/CH06PR15.txt", header=TRUE)

# scatter plot matrix
pairs(patient, col="slategray4")

# correlation matrix
cor(patient)

# fit linear model
gfit <- lm(Satisfaction ~ Age + Severity + Anxiety, data=patient)

# plot residuals against fitted values
patient.e <- resid(gfit)
plot(gfit$fitted.values, patient.e, col="slategray4", pch=18)

# plot against single variables
par(mfrow=c(2,2))
plot(patient$Age, patient.e, col="slategray4", pch=18)
plot(patient$Severity, patient.e, col="slategray4", pch=18)
plot(patient$Anxiety, patient.e, col="slategray4", pch=18)

# plot against interaction variables
plot(patient$Age*patient$Severity, patient.e, col="slategray4", pch=18)
plot(patient$Age*patient$Anxiety, patient.e, col="slategray4", pch=18)
plot(patient$Severity*patient$Anxiety, patient.e, col="slategray4", pch=18)

# f test for not all coefficients = 0
summary(gfit)
anova(gfit)
MSR <- (8275.4+480.9+364.2)/3
MSE <- 4248.8/42
F_stat <- MSR/MSE #f stat
compare <- qf(.90, 3, 42) #f*

# coefficient of multiple determination
SSR <- MSR*3
SSTO <- SSR+ (MSE*42)
R2 <- SSR/SSTO

# estimate mean Y
ci <- predict(gfit, newdata=data.frame(Age=35, Severity=45, Anxiety=2.2), interval="confidence", level=.90)

# predict Y
pi <- predict(gfit, newdata=data.frame(Age=35, Severity=45, Anxiety=2.2), interval="prediction", level=.90)

# f test for b3 not = 0
anova(gfit)
f_stat_x3 <- (364.2/101.2)
f4 <- qf(.975, 1, 42)

# f test for b2 and b3 not = 0
msr_2and3 <- (480.9+364.2)/2
f_star_2and3 <- msr_2and3/101.2

# ////////////////////////////////// Answers //////////////////////////////////
# F Test for all Coefficients
# 
# The alternatives are as follows:
# H_0: \beta_1 = \beta_2 = \beta_3 = 0
# H_a: not all \beta_k = 0
#
# If the f-statistic is greater than F*, I will reject the null hypothesis. If 
# it is less than or equal to `r compare`, I will fail to reject the null 
# hypothesis.
# 
# Because the f-statistic is greater than the f-value, I can reject the null 
# hypothesis and conclude that at least one of the beta terms is not equal to 
# zero.
#
# The p-value is 1.542e-10, which makes sense as it is less than .10, which was 
# the required level of significance in order to reject the null.
#
# /////////////////////////////////////////////////////////////////////////////
# Coefficient of Multiple Determination
# 
# The coefficient of multiple determination indicates that the inclusion of 
# these three predictor variables reduces the variability in Y by 68.21973%.
# 
# /////////////////////////////////////////////////////////////////////////////
# F Test for B_3
# 
# H_0: B_3=0
# H_a: B_3 \neq 0
# 
# Because F-statistic is NOT greater than F*, I fail to reject the null 
# hypothesis. The P-value is 0.06468, which does not meet a significance 
# level of .025.
# /////////////////////////////////////////////////////////////////////////////
# 
# The degree of marginal linear association between $Y$ and $X_1$ is reduced 
# for each predictor variable considered before $X_1$ is added. This makes 
# sense because the reduction in variability of Y has the opportunity to be 
# attributed to other variables first and will not be double-counted.
# 
# /////////////////////////////////////////////////////////////////////////////
# 
# SSR(X_2|X_1, X_3) has 1 DF
#
# /////////////////////////////////////////////////////////////////////////////
```

```{r Indicator Variables}
# read data: gpa1 (original) + gpa2 (indicator) = gpa (full dataset)
gpa1 <- read.table("/Users/elizabethehmcke/Documents/6450/Assignment_4/CH01PR19-1.txt", header=TRUE)
gpa2 <- read.table("/Users/elizabethehmcke/Documents/6450/Assignment_4/CH08PR16.txt", header=TRUE)
gpa <- cbind(gpa1, gpa2[1]) #combine

# fit linear model
gfit <- lm(GPA ~ ACT + Indicator, data=gpa)
summary(gfit)

# confidence interval for beta_2
qt <- qt(.975, 117)
ll <- -.0943 - (qt* .11997)
ul <- -.0943 + (qt* .11997)

# ////////////////////////////////// Answers //////////////////////////////////
# 
# The 95% confidence interval for $\beta_2$ is (`r ll`, `r ul`). We fail to 
# reject the null hypothesis, because 0 is contained in the confidence interval.
# This suggests there is no value added by including $X_2$ in the model.
#
# /////////////////////////////////////////////////////////////////////////////

# plot residuals against variable; color by indicator
gpa.e <- resid(gfit)
palette(c("gray", "red"))
test <- gpa$Indicator + 1
plot(gpa$ACT, gpa.e, col=test, pch=18, 
     main="Residual Plot Against ACT Score Separated by Indicator", 
     xlab="ACT Score", ylab="Residuals")
```

```{r Interaction Term}

#fit linear model
gfit_int <- lm(GPA ~ ACT + Indicator + ACT*Indicator, data=gpa)
summary(gfit_int)

# ////////////////////////////////// Answers //////////////////////////////////
# When no major field of concentration is indicated, the interaction term is 
# dropped. However, when a major field of concentration *is* indicated by the 
# student, the interaction coefficient contributes to the slope of the 
# regression line pertaining to students who had indicated their preference of 
# major, such that for one unit increase in ACT score, the student's freshman 
# GPA increased by 0.059488.
# /////////////////////////////////////////////////////////////////////////////

# test interaction term not = 0
t_star1 <- .062245/.026487
t1 <- qt(.975, 116)

# /////////////////////////////////////////////////////////////////////////////
# H_0: \beta_3 = 0
# H_a: \beta_3 \neq 0
#
# Because t*, `r t_star1`, is greater than t, `r t1`, we can reject the null 
# hypothesis and conclude that the interaction term adds value to the 
# regression function.
# /////////////////////////////////////////////////////////////////////////////
```

```{r Added Variable Plot}

# required package
library(car)

# fit linear model
gfit2 <- lm(Satisfaction ~ Age + Severity + Anxiety, data=patient)

# added variable plot
avPlots(gfit2)

# /////////////////////////////////////////////////////////////////////////////
# Upon visual inspection of the added variable plots, it appears that $X_1$, 
# which represents age, may have a linear relationship with patient 
# satisfaction, while the relationship between $X_2$ and $X_3$ and patient 
# satisfaction remains unclear. It is possible these variables do not 
# significantly add value to the model.
# /////////////////////////////////////////////////////////////////////////////
```

```{r Outliers & Influential Points}
#uses patient data

# visual check for outliers (none should be found)
studres <- rstudent(gfit2)
plot(studres, pch=18, col='red', main="Studentized Residuals")
abline(h=0)

# identify 3 points with highest leverage (none should be found to be high)
# informally 0.5 is considered as a threshold for high leverage points
z <- hatvalues(gfit2)
sort(z, decreasing=TRUE)[1]
sort(z, decreasing=TRUE)[2]
sort(z, decreasing=TRUE)[3]

# check Cook's Distance for influential points (none should be found)
# Use F(0.5,p,n-p) as cutoff for influential point in Cook's distance.
library(lindia)
gg_cooksd(gfit2)
cutoff <- qf(.5, 4, 42)

```


```{r Piecewise Linear Model}

# read data
properties <- read.table("/Users/elizabethehmcke/Documents/MAS/AU/20/6450/Assignment_4/CH06PR18-1.txt", header=TRUE)

# center ages variable (square of variable included in model)
Centered_Ages <- properties$Age-mean(properties$Age)
Sq_Centered_Ages <- Centered_Ages^2

# create table
prop <- cbind(properties, Centered_Ages, Sq_Centered_Ages)

# plot scatter with vertical line at break
plot(prop$Age, prop$Rental_Rates, pch=18, col="red", ylab="Rental Rates", xlab="Property Age", main="Regression on X1")
abline(v=10, lty=2)

# fit piecewise model
piecewise.fit <- lm(Rental_Rates ~ Age + pmax(0, Age - 10) + I(Age > 10), data=prop)

# generate x variables for prediction
a.grid <- seq(0, max(prop$Age), length.out=1000)

# generate piecewise prediction lines
predictions <- predict(piecewise.fit, newdata=data.frame(Age = a.grid), interval = "confidence")

# add lines to graph (run entire chunk for success)
lines(x=a.grid, y=predictions[,1], col="gray3")
```

```{r Model Selection}

# required package for regsubsets()
library(leaps)

# read data
job <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 20/6450/Assignment_4/CH09PR10-1.txt", header=TRUE)

# select model; methods = exhaustive, forward, or backward
best_subs <- regsubsets(Job_Proficiency_Score ~ X_1 + X_2 + X_3 + X_4, 
                        data=job, 
                        method="exhaustive", 
                        nvmax=2)                    #include 2 variables max
summary(best_subs)
          
# adjusted r squared plot
plot(best_subs, scale="adjr2", main="Adjusted R^2 Plot")

# mallows cp plot
plot(best_subs, scale="Cp", main="Mallows Cp Plot")

# bic plot
plot(best_subs, scale="bic", main="BIC Plot")

# required package for step()
library(car)

# fit intercept and full model
gfit_int <- lm(Job_Proficiency_Score~1, data=job)
gfit_full <- lm(Job_Proficiency_Score~., data=job)

# aic model selection
forwstep_a <- step(gfit_int, scope=list(upper=gfit_full), data=job, direction="both")
forwstep_a

# bic model selection
forwstep_b <- step(gfit_int, scope=list(upper=gfit_full), data=job, direction="both", k=log(nrow(job)))
forwstep_b
```

```{r Check Model Fit}

# required package for PRESS()
library(MPV)

# fit PRESS statistic
fit_press <- PRESS(jobfit)/nrow(job)
anova(jobfit)

# read and fit linear model jobs prediction dataset
job2 <- read.table("/Users/elizabethehmcke/Documents/6450/Assignment_4/CH09PR22-1.txt", header=TRUE)
jobfit2 <- lm(Job_Proficiency_Score ~ X_3 + X_1 + X_4, data=job2)

# compare summary and anova (coefficients, SE, MSE, R squared)
summary(jobfit2)
summary(jobfit)
anova(jobfit2)
anova(jobfit)

# compare MSPE to MSE
pred_job2 <- predict(jobfit, newdata=job2)
mspe <- mean((job2$Job_Proficiency_Score-pred_job2)^2)
```

```{r Simple Logistic Model}

# plot logistic model with b0 = 20, b1 = -.2
X <- cbind(1:200)
pi_1a <- exp(20-.2*X)/(1+exp(20-.2*X))
plot(X, pi_1a, pch=18, col="slateblue1", main="Logistic Mean Response with Given Parameters")

# odds ratio is approximately exp(b1)
odds126 <- exp(20-.2*126)/(1+exp(20-.2*126))
odds125 <- exp(20-.2*125)/(1+exp(20-.2*125))
ratio <- odds126/odds125
exp(-.2)

# read data
dues <- read.table("/Users/elizabethehmcke/Documents/MAS/AU 21/6450/Assignment_5/CH14PR07.txt", header=TRUE)

# fit logistic model
gfit2 <- glm(Renewal ~ Increase, data=dues, family = binomial(link ="logit"))
summary(gfit2)

# /////////////////////////////////////////////////////////////////////////////
# The fitted response function is 
# \hat{\pi} = \frac{exp(-4.80751 + .12508X)}{1+exp(-4.80751 + .12508X)}
# /////////////////////////////////////////////////////////////////////////////

# generate prediction curve
predIncrease <- seq(0,100, by=1)                    # create x values
logit.hat2 <- predict.glm(gfit2,                    # ???
                          newdata = data.frame(Increase=predIncrease),
                          se.fit=TRUE)
prob.hat2 <- exp(logit.hat2$fit)/(1+exp(logit.hat2$fit)) # logistic equation

# plot original data
plot(dues$Increase, 
     dues$Renewal, 
     pch=18, col="slateblue1", 
     main="Annual Dues Increase", 
     ylab="Renewal Selection", 
     xlab="Increase Amount")

# plot loess smoothed line
loess2 <- loess.smooth(dues$Increase, dues$Renewal)
lines(loess2, col="slateblue2")                     # loess smooth line
lines(predIncrease, prob.hat2, col="red")           # predicted values line

# calculate and interpret odds ratio
beta_1 <- exp(gfit2$coefficients[2])

# /////////////////////////////////////////////////////////////////////////////
# The value is 1.133237 and suggests that for each additional dollar dues 
# increase, the odds of a member choosing not to renew their dues increases 13%.
# /////////////////////////////////////////////////////////////////////////////

# estimated probability that members will not renew if the dues increase $40
pi_2d <- exp(dues_b[1]+dues_b[2]*40)/(1+exp(dues_b[1]+dues_b[2]*40))

# dues increase for which 75 percent of the members are expected not to renew
dues_b <- gfit2$coefficients                        # save b1 and b0
Y <- exp(dues_b[1]+dues_b[2]*X)/(1+exp(dues_b[1]+dues_b[2]*X)) # equation
answer <- X[which(Y > .74 & Y < 0.76)]              # find Y = .75
```

